# CLAUDE.md (Consolidated Project Guidance for Claude)

This file provides comprehensive guidance for Claude AI when working with code in this repository. It consolidates project-specific setup, coding standards, testing methodologies, and overall project principles.

**Purpose:** Use this file as a primary reference for understanding project conventions, requirements, and best practices when generating or modifying code.

**Content Overview:**
1.  **Quick Start:** Build, setup, lint, and test commands.
2.  **Project Code Style Summary:** Key style guidelines for this specific project.
3.  **Detailed Coding Standards:** Comprehensive rules covering style, documentation, architecture, security, performance, and more.
4.  **Detailed Testing Manifesto:** In-depth testing principles and quality assurance standards.
5.  **Overall Project Standards Framework:** High-level view integrating development lifecycle, AI ethics, technical quality, and operations.
6.  **Master Prompts:** Pre-defined prompts for guiding LLM code generation based on these standards.

---

## 1. Quick Start: Build, Setup, Lint & Test Commands

### Build & Setup
```bash
# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install
```

### Lint & Test
```bash
# Format code
black .
isort .

# Lint code
flake8
mypy .

# Run all tests
pytest

# Run a single test
pytest tests/path/to/test_file.py::test_function_name

# Run tests with coverage
pytest --cov=src
```

---

## 2. Project Code Style Summary

* **Base Standard:** PEP 8.
* **Line Length:** Max 88 characters (Black default).
* **Naming:**
    * Variables, functions, methods: `snake_case` (e.g., `user_count`, `calculate_total()`).
    * Classes: `PascalCase` (e.g., `DependencyParser`, `RiskProfiler`).
    * Constants: `UPPER_SNAKE_CASE`.
* **Type Annotations:** Required for all function and method signatures.
* **Docstrings:** Use Google-style docstrings.
* **Function Size:** Aim for focused functions, ideally under 50 lines.
* **Exceptions:** Prefer custom exception classes for specific error conditions.
* **Imports:** Organize imports: standard library, third-party, local application (handled by `isort`).

---

## 3. Detailed Coding Standards

These standards provide comprehensive guidelines beyond the project summary.

### 3.1. Code Style and Formatting
1.  Follow established style guides for your language (e.g., Python: PEP 8).
2.  Enforce consistent formatting (indentation, line length (88 chars), statement termination, brackets, whitespace).
3.  Use meaningful naming conventions (PascalCase classes, snake\_case functions/variables, UPPER\_SNAKE\_CASE constants, underscore prefix for private).
4.  Structure code consistently (organize imports, group related functions/methods, consistent file organization, limit file/function size).
5.  Automate style enforcement (linters like flake8, formatters like black, pre-commit hooks, CI/CD integration).

### 3.2. Documentation Standards
1.  Include documentation for all public interfaces (purpose, parameters, returns, exceptions, examples).
2.  Add contextual documentation (module/file/class level, complex algorithms, rationale for decisions).
3.  Follow documentation format standards (consistent docstring format like Google-style, type hints, side effects, thread safety, performance notes).
4.  Maintain system-level documentation (architecture diagrams, flows, data models, APIs, deployment).
5.  Establish documentation review process (during code reviews, test accuracy, update with code, track coverage).

### 3.3. Architecture and Design Patterns
1.  Establish clear architectural boundaries (layers, separation of concerns, dependency inversion, module responsibilities, document decisions).
2.  Apply appropriate design patterns (Creational, Structural, Behavioral, Concurrency, Domain-specific).
3.  Follow SOLID principles (Single Responsibility, Open/Closed, Liskov Substitution, Interface Segregation, Dependency Inversion).
4.  Design for extensibility (interfaces, plugin systems, extension points, avoid tight coupling, feature toggles).
5.  Establish system boundaries (clear APIs, encapsulation, DDD where applicable, document constraints/assumptions, catalog technical debt).

### 3.4. Security Best Practices
1.  Apply input validation (validate at entry, sanitize, parameterized queries, safe parsers, type safety).
2.  Implement proper authentication (OAuth 2.0/OIDC, secure credential storage, MFA, session management, password policies).
3.  Apply appropriate authorization (RBAC/ABAC, check at each layer, least privilege, capability-based security, audit decisions).
4.  Protect sensitive data (encrypt at rest/transit, key management, minimize collection, retention policies, secure logging).
5.  Apply secure coding practices (secure defaults, proper error handling, memory safety, secure dependencies, security headers).

### 3.5. Performance Optimization
1.  Establish performance targets (response time, throughput, resource usage, latency, scaling).
2.  Apply algorithmic efficiency (appropriate data structures/algorithms, complexity analysis, avoid O(n^2+), memoization).
3.  Implement resource optimization (connection pooling, caching, memory management, I/O optimization, async processing).
4.  Apply database optimization (efficient schemas, indexes, optimized queries, query caching, DB-specific features).
5.  Implement proper benchmarking (automated tests, baselines, track metrics, profiling, continuous monitoring).

### 3.6. Error Handling
1.  Define error handling strategy (recoverable vs non-recoverable, exception hierarchies, retry policies, document behavior, reporting).
2.  Implement defensive programming (check preconditions, validate arguments, handle edge cases, design for failure, assertions).
3.  Create informative error messages (context, troubleshooting guidance, consistent format, error codes, localization).
4.  Apply proper exception handling (catch specific exceptions, avoid empty catch, maintain context, cleanup resources, log info).
5.  Implement structured logging (errors with stack traces, correlation IDs, severity levels, context-aware, avoid sensitive info).

### 3.7. Resource Management
1.  Apply proper resource lifecycle management (acquire late, release early, pooling, timeouts, circuit breakers).
2.  Handle external dependencies gracefully (fallbacks, progressive degradation, bulkheading, health checks, partial availability).
3.  Implement proper concurrency control (locking, synchronization, prevent deadlocks/races, pooling, non-blocking algorithms).
4.  Manage memory efficiently (cleanup, avoid leaks, resource limits, weak references, profile usage).
5.  Optimize file and network operations (buffer I/O, non-blocking I/O, connection pooling, batching, streaming).

### 3.8. Dependency Management
1.  Define dependency selection criteria (license, security, maintenance, community, compatibility).
2.  Implement version pinning (lock direct dependencies, specify ranges, document rationale, regular updates, automate updates).
3.  Apply dependency isolation (virtual environments, containerization, dependency injection, manage transitives, minimize footprint).
4.  Implement vulnerability scanning (CI/CD integration, security advisories, auto-updates, inventory, mitigation strategies).
5.  Create dependency documentation (purpose, mapping to features, alternatives analysis, upgrade paths, custom patches).

### 3.9. Version Control Practices (Git)
1.  Define branch management strategy (trunk-based/GitFlow, naming conventions, lifetimes, merge requirements, protection rules).
2.  Create commit standards (descriptive messages, conventional commits format, issue refs, atomic commits, sign commits).
3.  Implement code review workflows (pull/merge requests, review criteria, approvers, automated checks, review responsibilities).
4.  Apply versioning standards (semantic versioning, document breaking changes, changelogs, tag releases, archive artifacts).
5.  Establish repository hygiene (.gitignore, artifact storage, repo structure docs, hooks, repo documentation).

### 3.10. Code Review Standards
1.  Define review scope (correctness, standards adherence, security, performance, documentation).
2.  Establish review process (reviewers, size limits, timeframes, checklists, roles).
3.  Apply technical review criteria (algorithm correctness, error handling, test coverage, maintainability, compatibility).
4.  Implement review automation (style checks, static analysis, build/test verification, doc coverage, quality metrics).
5.  Foster constructive review culture (focus on code, actionable feedback, explain reasoning, ask questions, acknowledge good work).

### 3.11. Accessibility Standards (If Applicable)
1.  Apply semantic structure (HTML elements, headings, ARIA, labels, keyboard navigation).
2.  Implement responsive design (screen sizes, touch targets, flexible layouts, responsive images, multi-device testing).
3.  Apply color and contrast standards (WCAG contrast, don't rely on color alone, focus indicators, high contrast modes, simulators).
4.  Implement assistive technology support (alt text, transcripts, captions, screen reader support, testing).
5.  Apply accessibility testing (automated checkers, manual keyboard/screen reader testing, include in QA, document features).

### 3.12. Internationalization & Localization (If Applicable)
1.  Apply proper text externalization (extract strings, avoid concatenation, templates, pluralization, gender).
2.  Implement locale awareness (date/time/number formats, currencies, text directionality, sorting).
3.  Design for text expansion (flexible UI, avoid fixed width, test longer languages, dynamic layout, character sets).
4.  Implement resource management (organize by locale, fallbacks, efficient loading, locale switching, translation workflow).
5.  Apply localization testing (pseudo-localization, verify locales, bidirectional text, cultural appropriateness, native speaker testing).

### 3.13. Concurrency and Parallelism
1.  Define concurrency models (threading, async/await, actors, event-driven, data access patterns).
2.  Implement thread safety (document guarantees, use thread-safe collections, synchronization, immutability, atomics).
3.  Apply parallelism patterns (task/data/pipeline parallelism, work distribution, scale-out).
4.  Manage shared resources (document contention, locks, lock-free algorithms, resource limits, backpressure).
5.  Test concurrent code (race conditions, load testing, simulate slow resources, fuzzing, document assumptions).

### 3.14. API Design
1.  Apply API design principles (consistency, intuitiveness, least surprise, evolution, document decisions).
2.  Implement proper versioning (semantic, backward compatibility, document breaking changes, migration paths, graceful deprecation).
3.  Define interface contracts (behavior, constraints, errors, side effects, performance).
4.  Apply REST/GraphQL best practices (HTTP methods, naming, status codes, efficient queries, pagination/filtering).
5.  Implement API security (authentication, authorization, rate limiting, input validation, document requirements).

### 3.15. Refactoring Guidelines
1.  Define refactoring triggers (code smells, complexity, performance bottlenecks, tech debt, maintainability metrics).
2.  Establish refactoring processes (document behavior, create tests, small changes, review, verify preservation).
3.  Implement refactoring techniques (extract method/class, consolidate conditionals, composition over inheritance, patterns, simplify).
4.  Apply refactoring tools (IDE features, automated tools, static analysis, quality metrics, history).
5.  Document refactoring outcomes (measure improvements, update docs, lessons learned, debt reduction, new patterns).

### 3.16. Sustainability and Green Coding
1.  Optimize resource efficiency (CPU, memory, I/O, lazy loading, efficient algorithms).
2.  Apply energy-aware design (batch background ops, efficient polling, push vs pull, mobile battery, power profiles).
3.  Implement efficient data practices (minimize transfers, compression, caching, optimize media, efficient formats).
4.  Design for hardware efficiency (support older hardware, progressive enhancement, minimize intensive animations, hardware acceleration).
5.  Measure environmental impact (energy tracking, carbon footprint, green hosting, document improvements, include efficiency in metrics).

---

## 4. Detailed Testing Manifesto

Follow these principles and standards for comprehensive testing.

### 4.1 Core Testing Principles

#### 4.1.1. Hypothesis Tests for Behavior Validation
* Identify the core hypothesis for each function.
* Write tests defining clear expectations ("Given X, should do Y").
* Test positive, negative, and boundary cases.
* Verify error handling.
* Use descriptive test names.
```python
# Example
def test_user_authentication_valid_credentials():
    """HYPOTHESIS: Given valid credentials, authentication should succeed."""
    # Arrange
    valid_username = "test_user"
    valid_password = "correct_password"
    # Act
    result = authenticate_user(valid_username, valid_password)
    # Assert
    assert result.success is True
    assert result.error_message is None
```

#### 4.1.2. Regression Tests for Known Fail States
* For each bug fix, create a test recreating failure conditions.
* Document the original issue and fix verification.
* Include issue/ticket references.
* Maintain a dedicated regression suite.
```python
# Example
def test_calculation_with_zero_division_protection():
    """REGRESSION: Bug #1234 - Division by zero crash.
    Ensures safe_divide returns None instead of raising ZeroDivisionError."""
    # Arrange
    input_value = 10
    divisor = 0
    expected_result = None
    # Act
    result = safe_divide(input_value, divisor)
    # Assert
    assert result == expected_result
```

#### 4.1.3. Benchmark Tests with SLA Enforcement
* Define clear performance metrics and SLAs (latency, throughput, resources, errors).
* Create tests measuring against defined thresholds in controlled environments.
* Include average and percentile measurements (p95, p99).
* Alert on violations.
```python
# Example (Conceptual)
def test_api_response_time_sla():
    """BENCHMARK: API p95 < 200ms, p99 < 500ms, error rate < 0.1%."""
    # Arrange: Setup load generation
    # Act: Run N requests, collect timings and errors
    # Assert: Calculate p95, p99, error_rate and compare against SLA
    pass # Implementation depends on benchmark tool (e.g., pytest-benchmark, locust)
```

#### 4.1.4. Grammatical Evolution (GE) for Fuzzing + Edge Discovery
* Define a grammar (e.g., BNF) for valid system inputs.
* Implement an evolutionary algorithm to generate grammatically correct but potentially unexpected test cases.
* Use fitness functions prioritizing edge cases and coverage.
* Log failures and add discovered edge cases to regression tests.
```python
# Example (Conceptual)
def test_with_grammatical_evolution():
    """FUZZING: Use GE to discover edge cases in the input parser."""
    # Arrange: Define grammar, configure GE fuzzer
    # Act: Run fuzzer against target function (e.g., api_request_handler)
    # Assert: Check for critical failures, add findings to regression suite.
    pass # Requires a GE framework implementation
```

#### 4.1.5. Structured Logs for Agent Feedback
* Design structured logging (e.g., JSON) capturing inputs, outputs, decisions, confidence scores, metrics, deviations.
* Use consistent formats and correlation IDs.
* Implement levels for filtering.
* Create log analyzers.
```python
# Example Test
def test_agent_logging_completeness():
    """AGENT FEEDBACK: Verify agent produces comprehensive structured logs."""
    # Arrange: Define expected log fields, setup log capture
    # Act: Run agent process
    # Assert: Check logs exist, required fields are present, sequence complete, decisions logged.
    pass # Requires agent instrumentation and log capture mechanism
```

### 4.2 Quality Assurance Standards

#### 4.2.1. Code Coverage Requirements
* Thresholds: 85%+ overall line coverage, 95%+ critical components, 100% utilities.
* Track trends, prevent regression.
* Cover all paths, branches, error handling.
* Integrate reports in CI/CD, block merges below thresholds.
```python
# Example Config (e.g., pyproject.toml for pytest-cov)
# [tool.coverage.run]
# branch = true
# source = ["src"]
# [tool.coverage.report]
# fail_under = 85
# show_missing = true
# # Configure path-specific thresholds if tool supports it
```

#### 4.2.2. Static Analysis Rules
* Configure linters (flake8, pylint) and analyzers (mypy, bandit) with strict rules.
* Enforce style, find bugs, identify security/performance issues.
* Create project-specific custom rules.
* Integrate into pre-commit hooks and CI/CD.
* Maintain a "zero warnings" policy (treat warnings as errors).
```python
# Example pre-commit hook config (`.pre-commit-config.yaml`)
# - repo: [https://github.com/pycqa/flake8](https://github.com/pycqa/flake8)
#   rev: ...
#   hooks:
#   - id: flake8
#     additional_dependencies: [flake8-bugbear, flake8-comprehensions, ...]
# - repo: [https://github.com/pre-commit/mirrors-mypy](https://github.com/pre-commit/mirrors-mypy)
#   rev: ...
#   hooks:
#   - id: mypy
```

#### 4.2.3. Contract Testing Framework
* Define explicit contracts for service interfaces (request/response formats, errors, performance, versions).
* Implement consumer-driven contract tests (e.g., using Pact).
* Maintain a contract registry.
* Automate verification in CI/CD, alert on violations.
```python
# Example (Conceptual using Pact)
# Consumer Side: Define expectations, generate pact file.
# Provider Side: Verify pact file against provider implementation.
# CI: Publish pacts, trigger provider verification.
```

#### 4.2.4. Mutation Testing Guidelines
* Apply systematic code mutations (modify operators, boundaries, logic) to test test effectiveness.
* Establish mutation score thresholds (e.g., 80%+ overall, 90%+ critical).
* Integrate periodically into CI/CD or on significant changes.
* Review and address surviving mutants.
* Focus on high-value targets (business logic, security, error handling).
```python
# Example (Using mutmut)
# Run: mutmut run
# Check results: mutmut results
# Aim for high killed mutant score. Add tests for surviving mutants.
```

#### 4.2.5. Property-Based Testing Framework
* Define invariant properties the code must satisfy (reversibility, mathematical laws, business rules).
* Use libraries like Hypothesis to generate diverse test inputs automatically.
* Define explicit property assertions focusing on "what" not "how".
* Incorporate failure analysis and add specific regressions for found issues.
```python
# Example (Using Hypothesis)
from hypothesis import given, strategies as st

@given(st.lists(st.integers()))
def test_sort_idempotence(values):
    """PROPERTY: Sorting an already sorted list produces the same result."""
    once = sorted(values)
    twice = sorted(once)
    assert once == twice
```

### 4.3 Security and Resilience

#### 4.3.1. Security Testing Guidelines
* Apply multi-level security testing (SAST, DAST, IAST, dependency scanning).
* Test against standards (OWASP Top 10, CWE/SANS Top 25).
* Implement specific tests for common vulnerabilities (injection, auth bypass, data exposure, XXE, SSRF, misconfigs).
* Incorporate into CI/CD, block on critical findings, track security debt.
```python
# Example
def test_sql_injection_prevention():
    """Verify protection against SQL injection attacks using known vectors."""
    # Arrange: Define attack vectors
    # Act/Assert: Test vulnerable endpoints with attack vectors, ensure no data leak/unintended action.
    pass # Use security testing tools/frameworks where possible
```

#### 4.3.2. Resilience Testing Framework (Chaos Engineering)
* Design chaos experiments (dependency failures, network issues, resource exhaustion).
* Define steady-state hypotheses and acceptable degradation/recovery.
* Run controlled experiments in production-like environments with monitoring and abort conditions.
* Incorporate findings into architecture and add regression tests.
```python
# Example (Conceptual)
def test_resilience_to_database_failure():
    """Verify system resilience (e.g., fallback to cache) when DB is unavailable."""
    # Arrange: Define steady state check
    # Act: Simulate DB failure (using chaos tooling or mocks)
    # Assert: Verify system behavior (e.g., reads work from cache, writes fail gracefully)
    # Act: Restore DB connection
    # Assert: Verify system recovers to steady state within RTO.
```

### 4.4 Documentation and Integration

#### 4.4.1. Documentation Testing
* Test all code examples in documentation (extract and execute).
* Validate API documentation completeness against actual implementation.
* Test user journeys (tutorials, setup instructions).
* Automate doc tests in CI/CD, flag documentation drift.
```python
# Example (Using doctest or custom script)
# Extract code blocks from Markdown/RestructuredText.
# Execute each block and check output against expected results if specified.
```

#### 4.4.2. Integration Testing Patterns
* Define integration boundaries explicitly.
* Use appropriate test doubles (mocks, stubs, spies) or real instances strategically.
* Implement end-to-end test scenarios for key user journeys/workflows.
* Manage test environments effectively (containerization, state reset, parallelization).
```python
# Example
def test_user_registration_end_to_end():
    """Verify the complete user registration process across all components."""
    # Arrange: Setup necessary services (API, DB, Email service mock/test mode)
    # Act: Perform registration steps via API calls
    # Assert: Verify intermediate steps (e.g., email sent) and final state (e.g., user in DB, login works).
```

#### 4.4.3. Testability Guidelines
* Design for testability (dependency injection, separation of concerns, avoid global state, pure functions).
* Create testability interfaces (time/filesystem/network abstractions, randomness control).
* Implement testing hooks (instrumentation, state inspection).
* Include testability in code reviews, refactor hard-to-test code.
```python
# Example: Use Dependency Injection
class TestableService:
    # Inject dependencies instead of creating them internally
    def __init__(self, db_client, email_notifier):
        self.db_client = db_client
        self.email_notifier = email_notifier
    # ... methods using injected dependencies ...

# In tests, inject mock/stub implementations
mock_db = MockDbClient()
mock_email = MockEmailNotifier()
service = TestableService(mock_db, mock_email)
# Test service logic...
```

---

## 5. Overall Project Standards Framework

This outlines the high-level framework guiding the project:

1.  **Development Lifecycle:** Tailored methodologies for LLM projects, knowledge sharing for AI specifics, cross-functional team collaboration, documentation for models/data/systems.
2.  **AI Ethics and Governance:** Responsible AI guidelines, bias detection/mitigation, privacy standards, transparent decision documentation, human oversight implementation.
3.  **Technical Quality:** Testing standards (as detailed above), coding standards (as detailed above), data governance, AI system deployment standards, model performance monitoring/observability.
4.  **Operational Excellence:** Incident management for AI issues, MLOps practices, AI regulation compliance, AI interface UX standards, sustainability considerations.

*(This section provides context; detailed implementation is covered in Coding and Testing standards sections.)*

---

## 6. Master Prompts for LLM Interaction

Use these prompts as starting points when requesting code generation or modification:

### Master Prompt for Coding Standards Implementation
```
Generate code for [specific feature/module description].

**IMPORTANT CONTEXT:** Before starting, review the `PROJECT_PLAN.md` file located in the repository root. Ensure the code you generate aligns with the overall project vision, current phase objectives, and core principles outlined there. Keep the target audience and non-goals in mind when making implementation choices.

Follow these comprehensive coding standards:

1.  **Style and Structure:**
    * Follow Python PEP 8 style guide conventions with Black formatting (88 char lines).
    * Use meaningful, consistent naming (snake\_case for functions/vars, PascalCase for classes).
    * Document all public interfaces thoroughly using Google-style docstrings with type hints.
    * Limit function/method size ideally below 50 lines.
    * Apply consistent error handling using custom exceptions where appropriate.

2.  **Architecture and Design:**
    * Implement SOLID principles.
    * Apply appropriate design patterns where beneficial.
    * Define clear component boundaries and use dependency injection.
    * Design testable components.
    * Document significant architectural decisions.

3.  **Security and Performance:**
    * Validate all inputs thoroughly.
    * Apply proper authentication/authorization if relevant.
    * Optimize critical algorithms and resource usage.
    * Manage resources efficiently (e.g., use context managers).
    * Implement appropriate caching if needed.

4.  **Quality and Maintenance:**
    * Create comprehensive tests alongside the code (refer to Testing Manifesto in CLAUDE.md).
    * Document complex logic clearly.
    * Apply internationalization/accessibility standards if required by the feature.
    * Design for extensibility.

The code should be robust, efficient, secure, maintainable, directly contribute to the objectives in `PROJECT_PLAN.md`, and demonstrate high craftsmanship adhering to all project standards outlined in `CLAUDE.md`.
```

### Master Prompt for Test Suite Generation
```
Generate a comprehensive test suite for the provided code ([link/path to code file(s)]).

**IMPORTANT CONTEXT:** Before starting, review the `PROJECT_PLAN.md` file located in the repository root. Ensure the tests you generate verify that the code behaves according to the overall project vision, current phase objectives, and core principles outlined there. Test scenarios should reflect the target audience's usage patterns and respect defined non-goals.

Follow the project's Testing Manifesto (detailed in CLAUDE.md):

1.  **Core Testing Principles:**
    * Hypothesis tests validating core behaviors and expectations relevant to the project goals.
    * Regression tests covering known edge cases or previously fixed bugs.
    * Benchmark tests for performance-critical sections (if applicable, aligning with principles in `PROJECT_PLAN.md`).
    * Consider inputs for fuzzing/edge case discovery (if applicable).
    * Ensure code generates structured logs if it's an agent/process.

2.  **Quality Assurance:**
    * Aim for high code coverage (85%+ general, 95%+ critical).
    * Ensure code passes static analysis (flake8, mypy).
    * Implement contract tests if interacting with defined interfaces.
    * Consider property-based tests (using Hypothesis) for functions with clear invariants.

3.  **Security and Resilience:**
    * Include security tests relevant to project constraints and potential risks.
    * Add resilience tests if the code handles external dependencies or failure modes mentioned as constraints/principles.

4.  **Documentation and Integration:**
    * Create integration tests for interactions between components/modules if applicable to the current objectives.
    * Ensure tests are clear and serve as documentation.

For each test:
* Use descriptive names (`test_feature_scenario_expected_outcome`).
* Include clear docstrings explaining the test's purpose and its relevance to project goals if not obvious.
* Arrange preconditions, Act by calling the code, Assert expected outcomes.
* Use fixtures (e.g., pytest fixtures) for setup/teardown where appropriate.
* Categorize tests appropriately (unit, integration, etc. - potentially via markers).

The test suite should be maintainable, provide fast feedback, and verify the code's correctness, robustness, and alignment with the requirements and goals specified in both `CLAUDE.md` and `PROJECT_PLAN.md`.
```

---
**End of CLAUDE.md**